{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('https://pjreddie.com/media/files/mnist_train.csv',header=None)\n",
    "test_set = pd.read_csv('https://pjreddie.com/media/files/mnist_test.csv',header=None)\n",
    "\n",
    "#https://pjreddie.com/media/files/mnist_train.csv\n",
    "#https://pjreddie.com/media/files/mnist_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get labels in own array\n",
    "train_lb=np.array(train_set[0])\n",
    "test_lb=np.array(test_set[0])\n",
    "\n",
    "#one hot encode the labels\n",
    "train_lb=(np.arange(10) == train_lb[:,None]).astype(np.float32)\n",
    "test_lb=(np.arange(10) == test_lb[:,None]).astype(np.float32)\n",
    "\n",
    "#drop the labels column from training dataframe\n",
    "trainX=train_set.drop(0,axis=1)\n",
    "testX=test_set.drop(0,axis=1)\n",
    "\n",
    "#put in correct float32 array format\n",
    "trainX=np.array(trainX).astype(np.float32)\n",
    "testX=np.array(testX).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformat the data so it's not flat\n",
    "trainX=trainX.reshape(len(trainX),28,28,1)\n",
    "testX = testX.reshape(len(testX),28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a validation set and remove it from the train set\n",
    "trainX,valX,train_lb,val_lb=trainX[0:(len(trainX)-500),:,:,:],trainX[(len(trainX)-500):len(trainX),:,:,:],\\\n",
    "                            train_lb[0:(len(trainX)-500),:],train_lb[(len(trainX)-500):len(trainX),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure the images are alright\n",
    "plt.imshow(trainX.reshape(len(trainX),28,28)[0],cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to batch the test data because running low on memory\n",
    "class test_batchs:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.batch_index = 0\n",
    "    def nextBatch(self,batch_size):\n",
    "        if (batch_size+self.batch_index) > self.data.shape[0]:\n",
    "            print (\"batch sized is messed up\")\n",
    "        batch = self.data[self.batch_index:(self.batch_index+batch_size),:,:,:]\n",
    "        self.batch_index= self.batch_index+batch_size\n",
    "        return batch\n",
    "\n",
    "#set the test batchsize\n",
    "test_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns accuracy of model\n",
    "def accuracy(target,predictions):\n",
    "    return(100.0*np.sum(np.argmax(target,1) == np.argmax(predictions,1))/target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use os to get our current working directory so we can save variable\n",
    "file_path = os.getcwd()+'/model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "map1 = 32\n",
    "map2 = 64\n",
    "num_fc1 = 700 #1028\n",
    "num_fc2 = 10\n",
    "reduce1x1 = 16\n",
    "dropout=0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #train data and labels\n",
    "    X = tf.placeholder(tf.float32,shape=(batch_size,28,28,1))\n",
    "    y_ = tf.placeholder(tf.float32,shape=(batch_size,10))\n",
    "    \n",
    "    #validation data\n",
    "    tf_valX = tf.placeholder(tf.float32,shape=(len(valX),28,28,1))\n",
    "    \n",
    "    #test data\n",
    "    tf_testX=tf.placeholder(tf.float32,shape=(test_batch_size,28,28,1))\n",
    "    \n",
    "    def createWeight(size,Name):\n",
    "        return tf.Variable(tf.truncated_normal(size, stddev=0.1),\n",
    "                          name=Name)\n",
    "    \n",
    "    def createBias(size,Name):\n",
    "        return tf.Variable(tf.constant(0.1,shape=size),\n",
    "                          name=Name)\n",
    "    \n",
    "    def conv2d_s1(x,W):\n",
    "        return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "    \n",
    "    def max_pool_3x3_s1(x):\n",
    "        return tf.nn.max_pool(x,ksize=[1,3,3,1],\n",
    "                             strides=[1,1,1,1],padding='SAME')\n",
    "    \n",
    "    \n",
    "    #Inception Module1\n",
    "    #\n",
    "    #follows input\n",
    "    W_conv1_1x1_1 = createWeight([1,1,1,map1],'W_conv1_1x1_1')\n",
    "    b_conv1_1x1_1 = createWeight([map1],'b_conv1_1x1_1')\n",
    "    \n",
    "    #follows input\n",
    "    W_conv1_1x1_2 = createWeight([1,1,1,reduce1x1],'W_conv1_1x1_2')\n",
    "    b_conv1_1x1_2 = createWeight([reduce1x1],'b_conv1_1x1_2')\n",
    "    \n",
    "    #follows input\n",
    "    W_conv1_1x1_3 = createWeight([1,1,1,reduce1x1],'W_conv1_1x1_3')\n",
    "    b_conv1_1x1_3 = createWeight([reduce1x1],'b_conv1_1x1_3')\n",
    "    \n",
    "    #follows 1x1_2\n",
    "    W_conv1_3x3 = createWeight([3,3,reduce1x1,map1],'W_conv1_3x3')\n",
    "    b_conv1_3x3 = createWeight([map1],'b_conv1_3x3')\n",
    "    \n",
    "    #follows 1x1_3\n",
    "    W_conv1_5x5 = createWeight([5,5,reduce1x1,map1],'W_conv1_5x5')\n",
    "    b_conv1_5x5 = createBias([map1],'b_conv1_5x5')\n",
    "    \n",
    "    #follows max pooling\n",
    "    W_conv1_1x1_4= createWeight([1,1,1,map1],'W_conv1_1x1_4')\n",
    "    b_conv1_1x1_4= createWeight([map1],'b_conv1_1x1_4')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Inception Module2\n",
    "    #\n",
    "    #follows inception1\n",
    "    W_conv2_1x1_1 = createWeight([1,1,4*map1,map2],'W_conv2_1x1_1')\n",
    "    b_conv2_1x1_1 = createWeight([map2],'b_conv2_1x1_1')\n",
    "    \n",
    "    #follows inception1\n",
    "    W_conv2_1x1_2 = createWeight([1,1,4*map1,reduce1x1],'W_conv2_1x1_2')\n",
    "    b_conv2_1x1_2 = createWeight([reduce1x1],'b_conv2_1x1_2')\n",
    "    \n",
    "    #follows inception1\n",
    "    W_conv2_1x1_3 = createWeight([1,1,4*map1,reduce1x1],'W_conv2_1x1_3')\n",
    "    b_conv2_1x1_3 = createWeight([reduce1x1],'b_conv2_1x1_3')\n",
    "    \n",
    "    #follows 1x1_2\n",
    "    W_conv2_3x3 = createWeight([3,3,reduce1x1,map2],'W_conv2_3x3')\n",
    "    b_conv2_3x3 = createWeight([map2],'b_conv2_3x3')\n",
    "    \n",
    "    #follows 1x1_3\n",
    "    W_conv2_5x5 = createWeight([5,5,reduce1x1,map2],'W_conv2_5x5')\n",
    "    b_conv2_5x5 = createBias([map2],'b_conv2_5x5')\n",
    "    \n",
    "    #follows max pooling\n",
    "    W_conv2_1x1_4= createWeight([1,1,4*map1,map2],'W_conv2_1x1_4')\n",
    "    b_conv2_1x1_4= createWeight([map2],'b_conv2_1x1_4')\n",
    "    \n",
    "    #Fully connected layers\n",
    "    #since padding is same, the feature map with there will be 4 28*28*map2\n",
    "    W_fc1 = createWeight([28*28*(4*map2),num_fc1],'W_fc1')\n",
    "    b_fc1 = createBias([num_fc1],'b_fc1')\n",
    "    \n",
    "    W_fc2 = createWeight([num_fc1,num_fc2],'W_fc2')\n",
    "    b_fc2 = createBias([num_fc2],'b_fc2')\n",
    "\n",
    "    def model(x,train=True):\n",
    "        #Inception Module 1\n",
    "        conv1_1x1_1 = conv2d_s1(x,W_conv1_1x1_1)+b_conv1_1x1_1\n",
    "        conv1_1x1_2 = tf.nn.relu(conv2d_s1(x,W_conv1_1x1_2)+b_conv1_1x1_2)\n",
    "        conv1_1x1_3 = tf.nn.relu(conv2d_s1(x,W_conv1_1x1_3)+b_conv1_1x1_3)\n",
    "        conv1_3x3 = conv2d_s1(conv1_1x1_2,W_conv1_3x3)+b_conv1_3x3\n",
    "        conv1_5x5 = conv2d_s1(conv1_1x1_3,W_conv1_5x5)+b_conv1_5x5\n",
    "        maxpool1 = max_pool_3x3_s1(x)\n",
    "        conv1_1x1_4 = conv2d_s1(maxpool1,W_conv1_1x1_4)+b_conv1_1x1_4\n",
    "        \n",
    "        #concatenate all the feature maps and hit them with a relu\n",
    "        inception1 = tf.nn.relu(tf.concat(3,[conv1_1x1_1,conv1_3x3,conv1_5x5,conv1_1x1_4]))\n",
    "\n",
    "        \n",
    "        #Inception Module 2\n",
    "        conv2_1x1_1 = conv2d_s1(inception1,W_conv2_1x1_1)+b_conv2_1x1_1\n",
    "        conv2_1x1_2 = tf.nn.relu(conv2d_s1(inception1,W_conv2_1x1_2)+b_conv2_1x1_2)\n",
    "        conv2_1x1_3 = tf.nn.relu(conv2d_s1(inception1,W_conv2_1x1_3)+b_conv2_1x1_3)\n",
    "        conv2_3x3 = conv2d_s1(conv2_1x1_2,W_conv2_3x3)+b_conv2_3x3\n",
    "        conv2_5x5 = conv2d_s1(conv2_1x1_3,W_conv2_5x5)+b_conv2_5x5\n",
    "        maxpool2 = max_pool_3x3_s1(inception1)\n",
    "        conv2_1x1_4 = conv2d_s1(maxpool2,W_conv2_1x1_4)+b_conv2_1x1_4\n",
    "        \n",
    "        #concatenate all the feature maps and hit them with a relu\n",
    "        inception2 = tf.nn.relu(tf.concat(3,[conv2_1x1_1,conv2_3x3,conv2_5x5,conv2_1x1_4]))\n",
    "\n",
    "        #flatten features for fully connected layer\n",
    "        inception2_flat = tf.reshape(inception2,[-1,28*28*4*map2])\n",
    "        \n",
    "        #Fully connected layers\n",
    "        if train:\n",
    "            h_fc1 =tf.nn.dropout(tf.nn.relu(tf.matmul(inception2_flat,W_fc1)+b_fc1),dropout)\n",
    "        else:\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(inception2_flat,W_fc1)+b_fc1)\n",
    "\n",
    "        return tf.matmul(h_fc1,W_fc2)+b_fc2\n",
    "    \n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(model(X),y_))\n",
    "    opt = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    predictions_val = tf.nn.softmax(model(tf_valX,train=False))\n",
    "    predictions_test = tf.nn.softmax(model(tf_testX,train=False))\n",
    "    \n",
    "    #initialize variable\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    #use to save variables so we can pick up later\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 50\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "#initialize variables\n",
    "sess.run(init)\n",
    "print(\"Model initialized.\")\n",
    "\n",
    "#set use_previous=1 to use file_path model\n",
    "#set use_previous=0 to start model from scratch\n",
    "use_previous = 0\n",
    "\n",
    "#use the previous model or don't and initialize variables\n",
    "if use_previous:\n",
    "    saver.restore(sess,file_path)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "#training\n",
    "for s in range(num_steps):\n",
    "    offset = (s*batch_size) % (len(trainX)-batch_size)\n",
    "    batch_x,batch_y = trainX[offset:(offset+batch_size),:],train_lb[offset:(offset+batch_size),:]\n",
    "    feed_dict={X : batch_x, y_ : batch_y}\n",
    "    _,loss_value = sess.run([opt,loss],feed_dict=feed_dict)\n",
    "    if s%100 == 0:\n",
    "        feed_dict = {tf_valX : valX}\n",
    "        preds=sess.run(predictions_val,feed_dict=feed_dict)\n",
    "        \n",
    "        print (\"step: \"+str(s))\n",
    "        print (\"validation accuracy: \"+str(accuracy(val_lb,preds)))\n",
    "        print (\" \")\n",
    "        \n",
    "    #get test accuracy and save model\n",
    "    if s == (num_steps-1):\n",
    "        #create an array to store the outputs for the test\n",
    "        result = np.array([]).reshape(0,10)\n",
    "\n",
    "        #use the batches class\n",
    "        batch_testX=test_batchs(testX)\n",
    "\n",
    "        for i in range(len(testX)/test_batch_size):\n",
    "            feed_dict = {tf_testX : batch_testX.nextBatch(test_batch_size)}\n",
    "            preds=sess.run(predictions_test, feed_dict=feed_dict)\n",
    "            result=np.concatenate((result,preds),axis=0)\n",
    "        \n",
    "        print (\"test accuracy: \"+str(accuracy(test_lb,result)))\n",
    "        \n",
    "        save_path = saver.save(sess,file_path)\n",
    "        print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
