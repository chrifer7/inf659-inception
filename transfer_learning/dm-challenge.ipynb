{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install tqdm\n",
    "#!pip install progress\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.gridspec as gridspec\n",
    "#import cv2\n",
    "import dataset_utils\n",
    "#import inception_preprocessing\n",
    "import inception_v3 as v3\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "#from getvector import getvector\n",
    "from tensorflow.python.platform import gfile\n",
    "from progress.bar import Bar\n",
    "from math import floor, ceil, pi\n",
    "\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de datos aumentados y extracción de características usando InceptionV3\n",
    "\n",
    "En esta sección se realiza lo siguiente:\n",
    "- Carga de mamografías según su clase.\n",
    "- Aumentación de datos (flip horizontal, zoom)\n",
    "- Extracción de vector de características usando InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para aumentación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "#Ploteo de transformación\n",
    "def plot_transform_steps(fig, tensor_image, fils = 1, cols = 1, pos = 1, title = \"\", show = False):\n",
    "    with tf.Session() as sess:\n",
    "        im = sess.run(tensor_image)\n",
    "        #print(im)\n",
    "        im = (im * 128 + 128) / 256\n",
    "        \n",
    "        ax = fig.add_subplot(fils, cols, pos)\n",
    "        plt.subplots_adjust(wspace=0.2, hspace=0.4)\n",
    "        \n",
    "        ax.imshow(im)\n",
    "        ax.set_title(title)\n",
    "        \n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def apply_with_random_selector(x, func, num_cases):\n",
    "  \"\"\"Computes func(x, sel), with sel sampled from [0...num_cases-1].\n",
    "\n",
    "  Args:\n",
    "    x: input Tensor.\n",
    "    func: Python function to apply.\n",
    "    num_cases: Python int32, number of cases to sample sel from.\n",
    "\n",
    "  Returns:\n",
    "    The result of func(x, sel), where func receives the value of the\n",
    "    selector as a python integer, but sel is sampled dynamically.\n",
    "  \"\"\"\n",
    "  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n",
    "  # Pass the real x only to one of the func calls.\n",
    "  return control_flow_ops.merge([\n",
    "      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n",
    "      for case in range(num_cases)])[0]\n",
    "\n",
    "\n",
    "def distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n",
    "  \"\"\"Distort the color of a Tensor image.\n",
    "\n",
    "  Each color distortion is non-commutative and thus ordering of the color ops\n",
    "  matters. Ideally we would randomly permute the ordering of the color ops.\n",
    "  Rather then adding that level of complication, we select a distinct ordering\n",
    "  of color ops for each preprocessing thread.\n",
    "\n",
    "  Args:\n",
    "    image: 3-D Tensor containing single image in [0, 1].\n",
    "    color_ordering: Python int, a type of distortion (valid values: 0-3).\n",
    "    fast_mode: Avoids slower ops (random_hue and random_contrast)\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    3-D Tensor color-distorted image on range [0, 1]\n",
    "  Raises:\n",
    "    ValueError: if color_ordering not in [0, 3]\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'distort_color', [image]):\n",
    "    if fast_mode:\n",
    "      if color_ordering == 0:\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "      else:\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "    else:\n",
    "      if color_ordering == 0:\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_hue(image, max_delta=0.2)\n",
    "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "      elif color_ordering == 1:\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_hue(image, max_delta=0.2)\n",
    "      elif color_ordering == 2:\n",
    "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_hue(image, max_delta=0.2)\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "      elif color_ordering == 3:\n",
    "        image = tf.image.random_hue(image, max_delta=0.2)\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "      else:\n",
    "        raise ValueError('color_ordering must be in [0, 3]')\n",
    "\n",
    "    # The random_* ops do not necessarily clamp.\n",
    "    return tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def distorted_bounding_box_crop(image,\n",
    "                                bbox,\n",
    "                                min_object_covered=0.1,\n",
    "                                aspect_ratio_range=(0.75, 1.33),\n",
    "                                area_range=(0.05, 1.0),\n",
    "                                max_attempts=100,\n",
    "                                scope=None):\n",
    "  \"\"\"Generates cropped_image using a one of the bboxes randomly distorted.\n",
    "\n",
    "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
    "\n",
    "  Args:\n",
    "    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n",
    "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "      where each coordinate is [0, 1) and the coordinates are arranged\n",
    "      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n",
    "      image.\n",
    "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
    "      area of the image must contain at least this fraction of any bounding box\n",
    "      supplied.\n",
    "    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n",
    "      image must have an aspect ratio = width / height within this range.\n",
    "    area_range: An optional list of `floats`. The cropped area of the image\n",
    "      must contain a fraction of the supplied image within in this range.\n",
    "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
    "      region of the image of the specified constraints. After `max_attempts`\n",
    "      failures, return the entire image.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
    "    # Each bounding box has shape [1, num_boxes, box coords] and\n",
    "    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n",
    "\n",
    "    # A large fraction of image datasets contain a human-annotated bounding\n",
    "    # box delineating the region of the image containing the object of interest.\n",
    "    # We choose to create a new bounding box for the object which is a randomly\n",
    "    # distorted version of the human-annotated bounding box that obeys an\n",
    "    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n",
    "    # bounding box. If no box is supplied, then we assume the bounding box is\n",
    "    # the entire image.\n",
    "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
    "        tf.shape(image),\n",
    "        bounding_boxes=bbox,\n",
    "        min_object_covered=min_object_covered,\n",
    "        aspect_ratio_range=aspect_ratio_range,\n",
    "        area_range=area_range,\n",
    "        max_attempts=max_attempts,\n",
    "        use_image_if_no_bounding_boxes=True)\n",
    "    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n",
    "    return cropped_image, distort_bbox\n",
    "\n",
    "\n",
    "def preprocess_for_train(image, height, width, bbox, _plot_image, imagedir,\n",
    "                         fast_mode=True,\n",
    "                         scope=None):\n",
    "  \"\"\"Distort one image for training a network.\n",
    "\n",
    "  Distorting images provides a useful technique for augmenting the data\n",
    "  set during training in order to make the network invariant to aspects\n",
    "  of the image that do not effect the label.\n",
    "\n",
    "  Additionally it would create image_summaries to display the different\n",
    "  transformations applied to the image.\n",
    "\n",
    "  Args:\n",
    "    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n",
    "      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n",
    "      is [0, MAX], where MAX is largest positive representable number for\n",
    "      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n",
    "    height: integer\n",
    "    width: integer\n",
    "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "      where each coordinate is [0, 1) and the coordinates are arranged\n",
    "      as [ymin, xmin, ymax, xmax].\n",
    "    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n",
    "      bi-cubic resizing, random_hue or random_contrast).\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    3-D float Tensor of distorted image used for training with range [-1, 1].\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'distort_image', [image, height, width, bbox]):\n",
    "    if bbox is None:\n",
    "      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n",
    "                         dtype=tf.float32,\n",
    "                         shape=[1, 1, 4])\n",
    "    if image.dtype != tf.float32:\n",
    "      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    # Each bounding box has shape [1, num_boxes, box coords] and\n",
    "    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n",
    "    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0), bbox)\n",
    "    \n",
    "    if _plot_image:\n",
    "        fig = plt.figure() #figsize = (2, 3)\n",
    "        fig.suptitle(\"Transformaciones aleatorias paso a paso: %s\"%imagedir, fontsize=14)\n",
    "    \n",
    "    #print(image_with_box)\n",
    "    if _plot_image:\n",
    "        plot_transform_steps(fig, image_with_box[0], 2, 3, 1, \"with_box\", False)\n",
    "    \n",
    "    tf.summary.image('image_with_bounding_boxes', image_with_box)\n",
    "\n",
    "    # Falta agregar rotación mínima aleatoria con cropping de fondo obligatorio\n",
    "\n",
    "    #Añadido para no recortar demasiado las mamografías\n",
    "    min_object_covered = 0.9\n",
    "    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox, min_object_covered)\n",
    "    # Restore the shape since the dynamic slice based upon the bbox_size loses\n",
    "    # the third dimension.\n",
    "    distorted_image.set_shape([None, None, 3])\n",
    "    image_with_distorted_box = tf.image.draw_bounding_boxes(\n",
    "        tf.expand_dims(image, 0), distorted_bbox)\n",
    "    tf.summary.image('images_with_distorted_bounding_box',\n",
    "                     image_with_distorted_box)\n",
    "    if _plot_image:\n",
    "        plot_transform_steps(fig, image_with_distorted_box[0], 2, 3, 2, \"distort box\", False)\n",
    "\n",
    "    # This resizing operation may distort the images because the aspect\n",
    "    # ratio is not respected. We select a resize method in a round robin\n",
    "    # fashion based on the thread number.\n",
    "    # Note that ResizeMethod contains 4 enumerated resizing methods.\n",
    "\n",
    "    # We select only 1 case for fast_mode bilinear.\n",
    "    num_resize_cases = 1 if fast_mode else 4\n",
    "    distorted_image = apply_with_random_selector(\n",
    "        distorted_image,\n",
    "        lambda x, method: tf.image.resize_images(x, [height, width], method=method),\n",
    "        num_cases=num_resize_cases)\n",
    "    \n",
    "    if _plot_image:\n",
    "        plot_transform_steps(fig, distorted_image, 2, 3, 3, \"resize met\", False)\n",
    "\n",
    "    tf.summary.image('cropped_resized_image',\n",
    "                     tf.expand_dims(distorted_image, 0))\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "    \n",
    "    if _plot_image:\n",
    "        plot_transform_steps(fig, distorted_image, 2, 3, 4, \"hor flip\", False)\n",
    "\n",
    "    # Randomly distort the colors. There are 4 ways to do it.\n",
    "    distorted_image = apply_with_random_selector(\n",
    "        distorted_image,\n",
    "        lambda x, ordering: distort_color(x, ordering, fast_mode),\n",
    "        num_cases=4)\n",
    "    \n",
    "    if _plot_image:\n",
    "        plot_transform_steps(fig, distorted_image, 2, 3, 5, \"dist colors\", False)\n",
    "\n",
    "    tf.summary.image('final_distorted_image',\n",
    "                     tf.expand_dims(distorted_image, 0))\n",
    "    distorted_image = tf.subtract(distorted_image, 0.5)\n",
    "    \n",
    "    if _plot_image:\n",
    "        plot_transform_steps(fig, distorted_image, 2, 3, 6, \"substract\", True)\n",
    "    \n",
    "    distorted_image = tf.multiply(distorted_image, 2.0)\n",
    "    \n",
    "    #print (distorted_image)\n",
    "    return distorted_image\n",
    "\n",
    "\n",
    "def preprocess_for_eval(image, height, width,\n",
    "                        central_fraction=0.875, scope=None):\n",
    "  \"\"\"Prepare one image for evaluation.\n",
    "\n",
    "  If height and width are specified it would output an image with that size by\n",
    "  applying resize_bilinear.\n",
    "\n",
    "  If central_fraction is specified it would cropt the central fraction of the\n",
    "  input image.\n",
    "\n",
    "  Args:\n",
    "    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n",
    "      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n",
    "      is [0, MAX], where MAX is largest positive representable number for\n",
    "      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n",
    "    height: integer\n",
    "    width: integer\n",
    "    central_fraction: Optional Float, fraction of the image to crop.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    3-D float Tensor of prepared image.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'eval_image', [image, height, width]):\n",
    "    if image.dtype != tf.float32:\n",
    "      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    # Crop the central region of the image with an area containing 87.5% of\n",
    "    # the original image.\n",
    "    if central_fraction:\n",
    "      image = tf.image.central_crop(image, central_fraction=central_fraction)\n",
    "\n",
    "    if height and width:\n",
    "      # Resize the image to the specified height and width.\n",
    "      image = tf.expand_dims(image, 0)\n",
    "      image = tf.image.resize_bilinear(image, [height, width],\n",
    "                                       align_corners=False)\n",
    "      image = tf.squeeze(image, [0])\n",
    "    image = tf.subtract(image, 0.5)\n",
    "    image = tf.multiply(image, 2.0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_image(image, height, width,                     \n",
    "                     is_training=False,\n",
    "                     _plot_image=False, imagedir = '',\n",
    "                     bbox=None,\n",
    "                     fast_mode=True):\n",
    "  \"\"\"Pre-process one image for training or evaluation.\n",
    "\n",
    "  Args:\n",
    "    image: 3-D Tensor [height, width, channels] with the image.\n",
    "    height: integer, image expected height.\n",
    "    width: integer, image expected width.\n",
    "    is_training: Boolean. If true it would transform an image for train,\n",
    "      otherwise it would transform it for evaluation.\n",
    "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "      where each coordinate is [0, 1) and the coordinates are arranged as\n",
    "      [ymin, xmin, ymax, xmax].\n",
    "    fast_mode: Optional boolean, if True avoids slower transformations.\n",
    "\n",
    "  Returns:\n",
    "    3-D float Tensor containing an appropriately scaled image\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if user does not provide bounding box\n",
    "  \"\"\"\n",
    "  if is_training:\n",
    "    #print (\"Preprocess for Training...\")\n",
    "    return preprocess_for_train(image, height, width, bbox, _plot_image, imagedir, fast_mode)\n",
    "  else:\n",
    "    #print (\"Preprocess for Evaluation...\")  \n",
    "    return preprocess_for_eval(image, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para Extracción de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vector que extrae las características\n",
    "def getvector(imagedir, is_training = False, image_type = 'jpg', plot_image = False):\n",
    "    slim = tf.contrib.slim\n",
    "    \n",
    "    batch_size = 3\n",
    "    image_size = v3.inception_v3.default_image_size\n",
    "\n",
    "    url = \"http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\"\n",
    "    checkpoints_dir = os.getcwd() + '/checkpoints'\n",
    "\n",
    "    if not tf.gfile.Exists(checkpoints_dir + '/inception_v3.ckpt'):\n",
    "        dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        # imagedir = '/home/jiexun/Desktop/Siraj/ImageChallenge/Necessary/train/cat.0.jpg'\n",
    "        image_string = tf.read_file(imagedir)\n",
    "        \n",
    "        if (image_type.lower() == 'jpg'):\n",
    "            image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        elif (image_type.lower() == 'png'):\n",
    "            image = tf.image.decode_png(image_string, channels=3)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        #processed_image = inception_preprocessing.preprocess_image(image, image_size, image_size, is_training)\n",
    "        processed_image = preprocess_image(image, image_size, image_size, is_training, plot_image, imagedir)\n",
    "        processed_images = tf.expand_dims(processed_image, 0)\n",
    "\n",
    "        # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "        #print('Inicializando el modelo InceptionV3...')\n",
    "        with slim.arg_scope(v3.inception_v3_arg_scope()):\n",
    "            num_classes=1001\n",
    "            vector, _ = v3.inception_v3(processed_images, num_classes, is_training)\n",
    "        \n",
    "        \n",
    "        init_fn = slim.assign_from_checkpoint_fn(os.path.join(checkpoints_dir, 'inception_v3.ckpt'),\n",
    "                                                 slim.get_model_variables('InceptionV3'))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            init_fn(sess)\n",
    "            np_image, vector = sess.run([image, vector])\n",
    "            \n",
    "            if plot_image:\n",
    "                fig = plt.figure()\n",
    "                fig.suptitle(\"Transformación a mamografía %s\"%imagedir, fontsize=14)\n",
    "\n",
    "                #Plotea la imagen original\n",
    "                ax1 = fig.add_subplot(1,2,1)\n",
    "                ax1.imshow(np_image)\n",
    "                ax1.set_title(\"Original image\")\n",
    "                #plt.show()\n",
    "\n",
    "                #Plotea la imagen procesada            \n",
    "                im = sess.run(processed_image)\n",
    "                im = (im * 128 + 128) / 256\n",
    "                #print(im)\n",
    "                ax2 = fig.add_subplot(1,2,2)\n",
    "                ax2.imshow(im)\n",
    "                ax2.set_title(\"Processed image\")\n",
    "                #plt.show()\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "        a = np.asarray([x for xs in vector for xss in xs for xsss in xss for x in xsss])\n",
    "        np.reshape(a, (1, 2048))\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aumentación de datos y extracción de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados de un archivo.\n"
     ]
    }
   ],
   "source": [
    "#Procesamiento\n",
    "data_inputs = []\n",
    "data_labels = []\n",
    "factor_data_augmentation = 10\n",
    "n_of_files_to_plot = 5\n",
    "n_gen_samples_to_plot = 2\n",
    "\n",
    "# Checking if the 2048-dimensional vector representations of the training images are already available\n",
    "if os.path.isfile('./data/DM Images/data_inputs.txt') and os.path.isfile('./data/DM Images/data_labels.txt'):\n",
    "    data_inputs = np.loadtxt('./data/DM Images/data_inputs.txt')\n",
    "    data_labels = np.loadtxt('./data/DM Images/data_labels.txt')\n",
    "    print('Datos cargados de un archivo.')\n",
    "\n",
    "else: \n",
    "    #tf.reset_default_graph()\n",
    "    # add in your images here if you want to train the model on your own images\n",
    "    data_dir = 'data/DM Images/train'\n",
    "    \n",
    "    #Cuenta la cantidad de directorios/clases\n",
    "    n_classes = 0\n",
    "    for o in os.listdir(data_dir):\n",
    "        if not o.startswith('.'):\n",
    "            n_classes = n_classes + 1\n",
    "    \n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    \n",
    "    i_class = 0\n",
    "    for o in os.listdir(data_dir):\n",
    "        if not o.startswith('.'):     \n",
    "            class_name = os.path.join(o)\n",
    "            print(\"Clase: \", class_name, \"Índice: \",  i_class)\n",
    "\n",
    "            file_list = []\n",
    "            file_glob = os.path.join(data_dir, class_name, '*.jpg');\n",
    "            print('File glob: ', file_glob)\n",
    "            file_list.extend(gfile.Glob(file_glob))\n",
    "            print('Samples: ', len(file_list))\n",
    "\n",
    "            file_list = file_list[0:4]\n",
    "            #bar = Bar('Inception-V3 is processing images:', max=300)\n",
    "            #bar = Bar('Inception-V3 is processing images:', max=len(file_list))\n",
    "            \n",
    "            one_hot_row = np.zeros(n_classes)             \n",
    "            one_hot_row[i_class] = 1\n",
    "            print('one hot: ', one_hot_row)\n",
    "            \n",
    "            n_data_gen = len(file_list)*factor_data_augmentation\n",
    "            pbar = tqdm(total=n_data_gen)\n",
    "            pbar.set_description(\"\\nExtrayendo características de %i muestras generadas de %s\" % (n_data_gen, class_name))\n",
    "            i = 0 #para contar cuántas mamografías por cada clase ploteo\n",
    "            plot_image = False\n",
    "            for file_name in file_list:\n",
    "                #tqdm.write(\"Processing:  %s\" % file_name)\n",
    "                #Aumentación de datos y extracción de características\n",
    "                if (i < n_of_files_to_plot):\n",
    "                        plot_image = True                        \n",
    "             \n",
    "                j = 0\n",
    "                for i in range(factor_data_augmentation):  \n",
    "                    if (i < n_gen_samples_to_plot):\n",
    "                        plot_image = True  \n",
    "                    data_inputs.append(getvector(file_name, True, 'jpg', plot_image))\n",
    "                    data_labels.append(one_hot_row)\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    plot_image = False\n",
    "                    j = j + 1\n",
    "                    \n",
    "                plot_image = False\n",
    "                i = i + 1\n",
    "                #bar.next()\n",
    "                \n",
    "            i_class = i_class + 1\n",
    "            pbar.close()\n",
    "    #bar.finish()\n",
    "    \n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "    tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "    \n",
    "    np.savetxt('data/DM Images/data_inputs.txt', data_inputs)\n",
    "    np.savetxt('data/DM Images/data_labels.txt', data_labels)\n",
    "    \n",
    "    print('Datos grabados en un archivo.')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "### Se configuran los parámetros iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting into train, val, and test\n",
    "train_inputs, valtest_inputs, train_labels, valtest_labels = train_test_split(data_inputs, data_labels, test_size=0.3, random_state=42)\n",
    "val_inputs, test_inputs, val_labels, test_labels = train_test_split(valtest_inputs, valtest_labels, test_size=0.4, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "log_batch_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful info\n",
    "n_features = np.size(train_inputs, 1)\n",
    "n_labels = np.size(train_labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders for input features and labels\n",
    "inputs = tf.placeholder(tf.float32, (None, n_features))\n",
    "labels = tf.placeholder(tf.float32, (None, n_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels), stddev=0.1), name='weights')\n",
    "bias = tf.Variable(tf.zeros(n_labels), name='bias')\n",
    "tf.add_to_collection('vars', weights)\n",
    "tf.add_to_collection('vars', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up operation in fully connected layer\n",
    "logits = tf.add(tf.matmul(inputs, weights), bias)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "tf.add_to_collection('pred', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining loss of network\n",
    "difference = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "loss = tf.reduce_sum(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting optimiser\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define accuracy\n",
    "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver((weights, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga del modelo InceptionV3\n",
    "\n",
    "Aquí se debe realizar el transfer learning\n",
    "(congelar las últimas capas del modelo y entrenar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1/100 : 100%|██████████| 1/1 [00:00<00:00, 131.04batches/s]\n",
      "Epoch  2/100 : 100%|██████████| 1/1 [00:00<00:00, 758.19batches/s]\n",
      "Epoch  3/100 : 100%|██████████| 1/1 [00:00<00:00, 828.42batches/s]\n",
      "Epoch  4/100 : 100%|██████████| 1/1 [00:00<00:00, 743.54batches/s]\n",
      "Epoch  5/100 : 100%|██████████| 1/1 [00:00<00:00, 668.41batches/s]\n",
      "Epoch  6/100 : 100%|██████████| 1/1 [00:00<00:00, 740.78batches/s]\n",
      "Epoch  7/100 : 100%|██████████| 1/1 [00:00<00:00, 664.50batches/s]\n",
      "Epoch  8/100 : 100%|██████████| 1/1 [00:00<00:00, 718.45batches/s]\n",
      "Epoch  9/100 : 100%|██████████| 1/1 [00:00<00:00, 704.33batches/s]\n",
      "Epoch 10/100 : 100%|██████████| 1/1 [00:00<00:00, 662.29batches/s]\n",
      "Epoch 11/100 : 100%|██████████| 1/1 [00:00<00:00, 573.38batches/s]\n",
      "Epoch 12/100 : 100%|██████████| 1/1 [00:00<00:00, 591.33batches/s]\n",
      "Epoch 13/100 : 100%|██████████| 1/1 [00:00<00:00, 616.54batches/s]\n",
      "Epoch 14/100 : 100%|██████████| 1/1 [00:00<00:00, 644.78batches/s]\n",
      "Epoch 15/100 : 100%|██████████| 1/1 [00:00<00:00, 656.59batches/s]\n",
      "Epoch 16/100 : 100%|██████████| 1/1 [00:00<00:00, 460.81batches/s]\n",
      "Epoch 17/100 : 100%|██████████| 1/1 [00:00<00:00, 589.25batches/s]\n",
      "Epoch 18/100 : 100%|██████████| 1/1 [00:00<00:00, 641.04batches/s]\n",
      "Epoch 19/100 : 100%|██████████| 1/1 [00:00<00:00, 584.82batches/s]\n",
      "Epoch 20/100 : 100%|██████████| 1/1 [00:00<00:00, 613.56batches/s]\n",
      "Epoch 21/100 : 100%|██████████| 1/1 [00:00<00:00, 409.52batches/s]\n",
      "Epoch 22/100 : 100%|██████████| 1/1 [00:00<00:00, 329.17batches/s]\n",
      "Epoch 23/100 : 100%|██████████| 1/1 [00:00<00:00, 676.61batches/s]\n",
      "Epoch 24/100 : 100%|██████████| 1/1 [00:00<00:00, 636.56batches/s]\n",
      "Epoch 25/100 : 100%|██████████| 1/1 [00:00<00:00, 337.05batches/s]\n",
      "Epoch 26/100 : 100%|██████████| 1/1 [00:00<00:00, 333.33batches/s]\n",
      "Epoch 27/100 : 100%|██████████| 1/1 [00:00<00:00, 341.89batches/s]\n",
      "Epoch 28/100 : 100%|██████████| 1/1 [00:00<00:00, 641.43batches/s]\n",
      "Epoch 29/100 : 100%|██████████| 1/1 [00:00<00:00, 479.18batches/s]\n",
      "Epoch 30/100 : 100%|██████████| 1/1 [00:00<00:00, 594.85batches/s]\n",
      "Epoch 31/100 : 100%|██████████| 1/1 [00:00<00:00, 703.27batches/s]\n",
      "Epoch 32/100 : 100%|██████████| 1/1 [00:00<00:00, 667.56batches/s]\n",
      "Epoch 33/100 : 100%|██████████| 1/1 [00:00<00:00, 735.20batches/s]\n",
      "Epoch 34/100 : 100%|██████████| 1/1 [00:00<00:00, 758.19batches/s]\n",
      "Epoch 35/100 : 100%|██████████| 1/1 [00:00<00:00, 672.92batches/s]\n",
      "Epoch 36/100 : 100%|██████████| 1/1 [00:00<00:00, 750.73batches/s]\n",
      "Epoch 37/100 : 100%|██████████| 1/1 [00:00<00:00, 533.02batches/s]\n",
      "Epoch 38/100 :   0%|          | 0/1 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 1, Loss: 64.087638855, Accuracy: 0.5\n",
      "After epoch 2, Loss: 52.0957641602, Accuracy: 0.5\n",
      "After epoch 3, Loss: 14.4108362198, Accuracy: 0.642857134342\n",
      "After epoch 4, Loss: 39.573184967, Accuracy: 0.5\n",
      "After epoch 5, Loss: 52.869758606, Accuracy: 0.5\n",
      "After epoch 6, Loss: 43.5869636536, Accuracy: 0.5\n",
      "After epoch 7, Loss: 20.0207309723, Accuracy: 0.5\n",
      "After epoch 8, Loss: 18.7999649048, Accuracy: 0.5\n",
      "After epoch 9, Loss: 33.3625068665, Accuracy: 0.5\n",
      "After epoch 10, Loss: 31.7587337494, Accuracy: 0.5\n",
      "After epoch 11, Loss: 18.0697975159, Accuracy: 0.5\n",
      "After epoch 12, Loss: 13.8846340179, Accuracy: 0.428571432829\n",
      "After epoch 13, Loss: 25.4811344147, Accuracy: 0.5\n",
      "After epoch 14, Loss: 27.6839408875, Accuracy: 0.5\n",
      "After epoch 15, Loss: 19.6403999329, Accuracy: 0.5\n",
      "After epoch 16, Loss: 11.8160762787, Accuracy: 0.357142865658\n",
      "After epoch 17, Loss: 17.9006481171, Accuracy: 0.5\n",
      "After epoch 18, Loss: 21.6165084839, Accuracy: 0.5\n",
      "After epoch 19, Loss: 16.9536113739, Accuracy: 0.5\n",
      "After epoch 20, Loss: 11.8166666031, Accuracy: 0.357142865658\n",
      "After epoch 21, Loss: 16.0074234009, Accuracy: 0.428571432829\n",
      "After epoch 22, Loss: 19.5496177673, Accuracy: 0.5\n",
      "After epoch 23, Loss: 16.6816310883, Accuracy: 0.5\n",
      "After epoch 24, Loss: 12.0876817703, Accuracy: 0.428571432829\n",
      "After epoch 25, Loss: 13.5723228455, Accuracy: 0.571428596973\n",
      "After epoch 26, Loss: 16.4774932861, Accuracy: 0.5\n",
      "After epoch 27, Loss: 15.0109148026, Accuracy: 0.642857134342\n",
      "After epoch 28, Loss: 12.0207433701, Accuracy: 0.5\n",
      "After epoch 29, Loss: 12.9543838501, Accuracy: 0.357142865658\n",
      "After epoch 30, Loss: 15.3523283005, Accuracy: 0.428571432829\n",
      "After epoch 31, Loss: 14.7961921692, Accuracy: 0.428571432829\n",
      "After epoch 32, Loss: 12.3914031982, Accuracy: 0.285714298487\n",
      "After epoch 33, Loss: 12.0754671097, Accuracy: 0.5\n",
      "After epoch 34, Loss: 13.7130479813, Accuracy: 0.571428596973\n",
      "After epoch 35, Loss: 14.0076618195, Accuracy: 0.642857134342\n",
      "After epoch 36, Loss: 12.5616197586, Accuracy: 0.571428596973\n",
      "After epoch 37, Loss: 11.8344688416, Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 : 100%|██████████| 1/1 [00:00<00:00, 421.24batches/s]\n",
      "Epoch 39/100 : 100%|██████████| 1/1 [00:00<00:00, 398.89batches/s]\n",
      "Epoch 40/100 : 100%|██████████| 1/1 [00:00<00:00, 709.22batches/s]\n",
      "Epoch 41/100 : 100%|██████████| 1/1 [00:00<00:00, 527.98batches/s]\n",
      "Epoch 42/100 : 100%|██████████| 1/1 [00:00<00:00, 593.84batches/s]\n",
      "Epoch 43/100 : 100%|██████████| 1/1 [00:00<00:00, 542.88batches/s]\n",
      "Epoch 44/100 : 100%|██████████| 1/1 [00:00<00:00, 549.50batches/s]\n",
      "Epoch 45/100 : 100%|██████████| 1/1 [00:00<00:00, 646.87batches/s]\n",
      "Epoch 46/100 : 100%|██████████| 1/1 [00:00<00:00, 813.01batches/s]\n",
      "Epoch 47/100 : 100%|██████████| 1/1 [00:00<00:00, 489.47batches/s]\n",
      "Epoch 48/100 : 100%|██████████| 1/1 [00:00<00:00, 520.51batches/s]\n",
      "Epoch 49/100 : 100%|██████████| 1/1 [00:00<00:00, 533.36batches/s]\n",
      "Epoch 50/100 : 100%|██████████| 1/1 [00:00<00:00, 326.28batches/s]\n",
      "Epoch 51/100 : 100%|██████████| 1/1 [00:00<00:00, 785.60batches/s]\n",
      "Epoch 52/100 : 100%|██████████| 1/1 [00:00<00:00, 751.26batches/s]\n",
      "Epoch 53/100 : 100%|██████████| 1/1 [00:00<00:00, 527.98batches/s]\n",
      "Epoch 54/100 : 100%|██████████| 1/1 [00:00<00:00, 594.85batches/s]\n",
      "Epoch 55/100 : 100%|██████████| 1/1 [00:00<00:00, 676.50batches/s]\n",
      "Epoch 56/100 : 100%|██████████| 1/1 [00:00<00:00, 554.95batches/s]\n",
      "Epoch 57/100 : 100%|██████████| 1/1 [00:00<00:00, 739.61batches/s]\n",
      "Epoch 58/100 : 100%|██████████| 1/1 [00:00<00:00, 655.77batches/s]\n",
      "Epoch 59/100 : 100%|██████████| 1/1 [00:00<00:00, 512.00batches/s]\n",
      "Epoch 60/100 : 100%|██████████| 1/1 [00:00<00:00, 486.13batches/s]\n",
      "Epoch 61/100 : 100%|██████████| 1/1 [00:00<00:00, 744.07batches/s]\n",
      "Epoch 62/100 : 100%|██████████| 1/1 [00:00<00:00, 724.15batches/s]\n",
      "Epoch 63/100 : 100%|██████████| 1/1 [00:00<00:00, 555.24batches/s]\n",
      "Epoch 64/100 : 100%|██████████| 1/1 [00:00<00:00, 484.27batches/s]\n",
      "Epoch 65/100 : 100%|██████████| 1/1 [00:00<00:00, 467.07batches/s]\n",
      "Epoch 66/100 : 100%|██████████| 1/1 [00:00<00:00, 483.55batches/s]\n",
      "Epoch 67/100 : 100%|██████████| 1/1 [00:00<00:00, 396.96batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 38, Loss: 12.7784614563, Accuracy: 0.357142865658\n",
      "After epoch 39, Loss: 13.5648908615, Accuracy: 0.357142865658\n",
      "After epoch 40, Loss: 12.9617404938, Accuracy: 0.357142865658\n",
      "After epoch 41, Loss: 11.9679660797, Accuracy: 0.428571432829\n",
      "After epoch 42, Loss: 11.9768304825, Accuracy: 0.5\n",
      "After epoch 43, Loss: 12.699877739, Accuracy: 0.571428596973\n",
      "After epoch 44, Loss: 12.9162998199, Accuracy: 0.571428596973\n",
      "After epoch 45, Loss: 12.3512115479, Accuracy: 0.571428596973\n",
      "After epoch 46, Loss: 11.8426513672, Accuracy: 0.428571432829\n",
      "After epoch 47, Loss: 11.9996128082, Accuracy: 0.428571432829\n",
      "After epoch 48, Loss: 12.4474744797, Accuracy: 0.357142865658\n",
      "After epoch 49, Loss: 12.5143318176, Accuracy: 0.357142865658\n",
      "After epoch 50, Loss: 12.1407833099, Accuracy: 0.357142865658\n",
      "After epoch 51, Loss: 11.8308010101, Accuracy: 0.5\n",
      "After epoch 52, Loss: 11.9222202301, Accuracy: 0.5\n",
      "After epoch 53, Loss: 12.2121620178, Accuracy: 0.571428596973\n",
      "After epoch 54, Loss: 12.3032093048, Accuracy: 0.571428596973\n",
      "After epoch 55, Loss: 12.1008052826, Accuracy: 0.5\n",
      "After epoch 56, Loss: 11.859752655, Accuracy: 0.5\n",
      "After epoch 57, Loss: 11.8263654709, Accuracy: 0.5\n",
      "After epoch 58, Loss: 11.969201088, Accuracy: 0.428571432829\n",
      "After epoch 59, Loss: 12.0731344223, Accuracy: 0.357142865658\n",
      "After epoch 60, Loss: 12.0121994019, Accuracy: 0.428571432829\n",
      "After epoch 61, Loss: 11.8688097, Accuracy: 0.5\n",
      "After epoch 62, Loss: 11.8036212921, Accuracy: 0.571428596973\n",
      "After epoch 63, Loss: 11.8686923981, Accuracy: 0.5\n",
      "After epoch 64, Loss: 11.9723701477, Accuracy: 0.5\n",
      "After epoch 65, Loss: 11.9981975555, Accuracy: 0.5\n",
      "After epoch 66, Loss: 11.9254817963, Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68/100 : 100%|██████████| 1/1 [00:00<00:00, 526.86batches/s]\n",
      "Epoch 69/100 : 100%|██████████| 1/1 [00:00<00:00, 547.06batches/s]\n",
      "Epoch 70/100 : 100%|██████████| 1/1 [00:00<00:00, 412.22batches/s]\n",
      "Epoch 71/100 : 100%|██████████| 1/1 [00:00<00:00, 491.89batches/s]\n",
      "Epoch 72/100 : 100%|██████████| 1/1 [00:00<00:00, 514.70batches/s]\n",
      "Epoch 73/100 : 100%|██████████| 1/1 [00:00<00:00, 520.26batches/s]\n",
      "Epoch 74/100 : 100%|██████████| 1/1 [00:00<00:00, 485.45batches/s]\n",
      "Epoch 75/100 : 100%|██████████| 1/1 [00:00<00:00, 481.50batches/s]\n",
      "Epoch 76/100 : 100%|██████████| 1/1 [00:00<00:00, 574.72batches/s]\n",
      "Epoch 77/100 : 100%|██████████| 1/1 [00:00<00:00, 539.67batches/s]\n",
      "Epoch 78/100 : 100%|██████████| 1/1 [00:00<00:00, 564.05batches/s]\n",
      "Epoch 79/100 : 100%|██████████| 1/1 [00:00<00:00, 628.93batches/s]\n",
      "Epoch 80/100 : 100%|██████████| 1/1 [00:00<00:00, 457.44batches/s]\n",
      "Epoch 81/100 : 100%|██████████| 1/1 [00:00<00:00, 464.69batches/s]\n",
      "Epoch 82/100 : 100%|██████████| 1/1 [00:00<00:00, 584.82batches/s]\n",
      "Epoch 83/100 : 100%|██████████| 1/1 [00:00<00:00, 574.09batches/s]\n",
      "Epoch 84/100 : 100%|██████████| 1/1 [00:00<00:00, 572.05batches/s]\n",
      "Epoch 85/100 : 100%|██████████| 1/1 [00:00<00:00, 530.52batches/s]\n",
      "Epoch 86/100 : 100%|██████████| 1/1 [00:00<00:00, 544.08batches/s]\n",
      "Epoch 87/100 : 100%|██████████| 1/1 [00:00<00:00, 643.10batches/s]\n",
      "Epoch 88/100 : 100%|██████████| 1/1 [00:00<00:00, 615.00batches/s]\n",
      "Epoch 89/100 : 100%|██████████| 1/1 [00:00<00:00, 471.01batches/s]\n",
      "Epoch 90/100 : 100%|██████████| 1/1 [00:00<00:00, 680.23batches/s]\n",
      "Epoch 91/100 : 100%|██████████| 1/1 [00:00<00:00, 535.06batches/s]\n",
      "Epoch 92/100 : 100%|██████████| 1/1 [00:00<00:00, 557.38batches/s]\n",
      "Epoch 93/100 : 100%|██████████| 1/1 [00:00<00:00, 420.86batches/s]\n",
      "Epoch 94/100 : 100%|██████████| 1/1 [00:00<00:00, 635.31batches/s]\n",
      "Epoch 95/100 : 100%|██████████| 1/1 [00:00<00:00, 566.57batches/s]\n",
      "Epoch 96/100 : 100%|██████████| 1/1 [00:00<00:00, 644.78batches/s]\n",
      "Epoch 97/100 : 100%|██████████| 1/1 [00:00<00:00, 321.55batches/s]\n",
      "Epoch 98/100 :   0%|          | 0/1 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 67, Loss: 11.8311710358, Accuracy: 0.428571432829\n",
      "After epoch 68, Loss: 11.7958278656, Accuracy: 0.571428596973\n",
      "After epoch 69, Loss: 11.8264856339, Accuracy: 0.5\n",
      "After epoch 70, Loss: 11.8668966293, Accuracy: 0.5\n",
      "After epoch 71, Loss: 11.8660030365, Accuracy: 0.5\n",
      "After epoch 72, Loss: 11.8267087936, Accuracy: 0.5\n",
      "After epoch 73, Loss: 11.7922306061, Accuracy: 0.571428596973\n",
      "After epoch 74, Loss: 11.7960615158, Accuracy: 0.5\n",
      "After epoch 75, Loss: 11.8307008743, Accuracy: 0.5\n",
      "After epoch 76, Loss: 11.8605937958, Accuracy: 0.5\n",
      "After epoch 77, Loss: 11.8585395813, Accuracy: 0.5\n",
      "After epoch 78, Loss: 11.8281383514, Accuracy: 0.5\n",
      "After epoch 79, Loss: 11.7949285507, Accuracy: 0.428571432829\n",
      "After epoch 80, Loss: 11.7804479599, Accuracy: 0.571428596973\n",
      "After epoch 81, Loss: 11.7853946686, Accuracy: 0.571428596973\n",
      "After epoch 82, Loss: 11.7945213318, Accuracy: 0.571428596973\n",
      "After epoch 83, Loss: 11.7940988541, Accuracy: 0.571428596973\n",
      "After epoch 84, Loss: 11.7840213776, Accuracy: 0.571428596973\n",
      "After epoch 85, Loss: 11.7750396729, Accuracy: 0.571428596973\n",
      "After epoch 86, Loss: 11.7766113281, Accuracy: 0.571428596973\n",
      "After epoch 87, Loss: 11.7880115509, Accuracy: 0.428571432829\n",
      "After epoch 88, Loss: 11.799823761, Accuracy: 0.428571432829\n",
      "After epoch 89, Loss: 11.8026695251, Accuracy: 0.5\n",
      "After epoch 90, Loss: 11.7946109772, Accuracy: 0.428571432829\n",
      "After epoch 91, Loss: 11.7813911438, Accuracy: 0.5\n",
      "After epoch 92, Loss: 11.7706785202, Accuracy: 0.571428596973\n",
      "After epoch 93, Loss: 11.7661561966, Accuracy: 0.571428596973\n",
      "After epoch 94, Loss: 11.7660865784, Accuracy: 0.571428596973\n",
      "After epoch 95, Loss: 11.7663974762, Accuracy: 0.571428596973\n",
      "After epoch 96, Loss: 11.7648353577, Accuracy: 0.571428596973\n",
      "After epoch 97, Loss: 11.7624797821, Accuracy: 0.571428596973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/100 : 100%|██████████| 1/1 [00:00<00:00, 346.75batches/s]\n",
      "Epoch 99/100 : 100%|██████████| 1/1 [00:00<00:00, 510.44batches/s]\n",
      "Epoch 100/100 : 100%|██████████| 1/1 [00:00<00:00, 612.04batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 98, Loss: 11.7619867325, Accuracy: 0.571428596973\n",
      "After epoch 99, Loss: 11.7647285461, Accuracy: 0.571428596973\n",
      "After epoch 100, Loss: 11.7693729401, Accuracy: 0.571428596973\n"
     ]
    }
   ],
   "source": [
    "# Run tensorflow session\n",
    "history = {'val_acc':[], 'val_loss':[]}\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Running the training in batches \n",
    "    batch_count = int(math.ceil(len(train_inputs)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{} '.format(epoch_i+1, epochs), unit='batches')\n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_inputs = train_inputs[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "            # Run optimizer\n",
    "            _ = sess.run(optimizer, feed_dict={inputs: batch_inputs, labels: batch_labels})\n",
    "\n",
    "        # Check accuracy against validation data\n",
    "        val_accuracy, val_loss = sess.run([accuracy, loss], feed_dict={inputs: val_inputs, labels: val_labels})\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(\"After epoch {}, Loss: {}, Accuracy: {}\".format(epoch_i+1, val_loss, val_accuracy))\n",
    "\n",
    "    g = tf.get_default_graph()\n",
    "    saver.save(sess, './sessions/v3')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.DataFrame(history)\n",
    "#display(df)\n",
    "\n",
    "df.plot(y=['val_loss'], figsize=(16,4), title='Loss')\n",
    "df.plot(y=['val_acc'], figsize=(16,4), title='Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas unitarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(prediction, file_name = ''):\n",
    "    #try: \n",
    "    #    file_name = 'cat.jpg'\n",
    "    #except IndexError:\n",
    "    #    print ('please enter image file path.........')\n",
    "    #    exit()\n",
    "    image_input = getvector(file_name).reshape((1,2048))\n",
    "    #if 'cat' in file_name:\n",
    "    #    image_label = [[1, 0]]plot_image\n",
    "    #else:\n",
    "    #    image_label = [[0, 1]]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        new_saver = tf.train.import_meta_graph('./sessions/v3')\n",
    "        new_saver.restore(sess, tf.train.latest_checkpoint('./')) #antes solo estaba ./\n",
    "\n",
    "        pred = sess.run(prediction, feed_dict={inputs: image_input})\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/christian/inf659-inception/transfer_learning/checkpoints/inception_v3.ckpt\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "File ./sessions/v3 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4c3a53923960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#import matplotlib.image as mpimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/DM Images/validation/Cancer/337671.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predict: Cancer/337671.jpg\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/DM Images/validation/Cancer/337671.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d089dbdc3f24>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(prediction, file_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mnew_saver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./sessions/v3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mnew_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#antes solo estaba ./\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/christian/tensordir/local/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1586\u001b[0m   \"\"\"\n\u001b[1;32m   1587\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/christian/tensordir/local/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.pyc\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    400\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ./sessions/v3 does not exist."
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.image as mpimg\n",
    "\n",
    "pred = predict(prediction, 'data/DM Images/validation/Cancer/337671.jpg')\n",
    "print('Predict: Cancer/337671.jpg\\n', )\n",
    "img=mpimg.imread('data/DM Images/validation/Cancer/337671.jpg')\n",
    "imgplot = plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print ('Cancer: {}, NoCancer: {}'.format(pred[0][0], pred[0][1]))\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "pred = predict(prediction, 'data/DM Images/validation/NoCancer/288567.jpg')\n",
    "print('Predict: NoCancer/288567.jpg\\n', )\n",
    "img=mpimg.imread('data/DM Images/validation/NoCancer/288567.jpg')\n",
    "imgplot = plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print ('Cancer: {}, NoCancer: {}'.format(pred[0][0], pred[0][1]))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
