{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install tqdm\n",
    "#!pip install progress\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from getvector import getvector\n",
    "from tensorflow.python.platform import gfile\n",
    "from progress.bar import Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_inputs = []\n",
    "data_labels = []\n",
    "\n",
    "# Checking if the 2048-dimensional vector representations of the training images are already available\n",
    "if os.path.isfile('./data_inputs.txt') and os.path.isfile('./data_labels.txt'):\n",
    "\tdata_inputs = np.loadtxt('data_inputs.txt')\n",
    "\tdata_labels = np.loadtxt('data_labels.txt')\n",
    "\n",
    "else: \n",
    "\t# add in your images here if you want to train the model on your own images\n",
    "\timage_dir = './train'\n",
    "\tfile_list = []\n",
    "\tfile_glob = os.path.join(image_dir, '*.jpg');\n",
    "\tfile_list.extend(gfile.Glob(file_glob))\n",
    "\n",
    "\t# I only used 300 images from the cats and dogs dataset\n",
    "\tfile_list = file_list[0:300]\n",
    "\tbar = Bar('Inception-V3 is processing images:', max=300)\n",
    "\tfor file_name in file_list:\n",
    "\t\tdata_inputs.append(getvector(file_name))\n",
    "\t\tif 'cat' in file_name:\n",
    "\t\t\tdata_labels.append([1, 0])\n",
    "\t\telse:\n",
    "\t\t\tdata_labels.append([0, 1])\n",
    "\t\tbar.next()\n",
    "\tbar.finish()\n",
    "\n",
    "\tnp.savetxt('data_inputs.txt', data_inputs)\n",
    "\tnp.savetxt('data_labels.txt', data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting into train, val, and test\n",
    "train_inputs, valtest_inputs, train_labels, valtest_labels = train_test_split(data_inputs, data_labels, test_size=0.3, random_state=42)\n",
    "val_inputs, test_inputs, val_labels, test_labels = train_test_split(valtest_inputs, valtest_labels, test_size=0.4, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "log_batch_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful info\n",
    "n_features = np.size(train_inputs, 1)\n",
    "n_labels = np.size(train_labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders for input features and labels\n",
    "inputs = tf.placeholder(tf.float32, (None, n_features))\n",
    "labels = tf.placeholder(tf.float32, (None, n_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels), stddev=0.1), name='weights')\n",
    "bias = tf.Variable(tf.zeros(n_labels), name='bias')\n",
    "tf.add_to_collection('vars', weights)\n",
    "tf.add_to_collection('vars', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up operation in fully connected layer\n",
    "logits = tf.add(tf.matmul(inputs, weights), bias)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "tf.add_to_collection('pred', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining loss of network\n",
    "difference = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "loss = tf.reduce_sum(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting optimiser\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define accuracy\n",
    "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver((weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1/20: 100%|██████████| 4/4 [00:00<00:00, 333.30batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 1, Loss: 28.175617218017578, Accuracy: 0.7222222089767456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2/20: 100%|██████████| 4/4 [00:00<00:00, 500.02batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 2, Loss: 2.382810592651367, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3/20: 100%|██████████| 4/4 [00:00<00:00, 499.87batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 3, Loss: 1.8865612745285034, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4/20: 100%|██████████| 4/4 [00:00<00:00, 999.83batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 4, Loss: 0.5509199500083923, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5/20: 100%|██████████| 4/4 [00:00<00:00, 999.95batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 5, Loss: 4.835564613342285, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6/20: 100%|██████████| 4/4 [00:00<00:00, 888.25batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 6, Loss: 4.357636451721191, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7/20: 100%|██████████| 4/4 [00:00<00:00, 925.44batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 7, Loss: 1.544984221458435, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8/20: 100%|██████████| 4/4 [00:00<00:00, 1000.07batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 8, Loss: 0.4649544954299927, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9/20: 100%|██████████| 4/4 [00:00<00:00, 1000.01batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 9, Loss: 0.2795045077800751, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 4/4 [00:00<00:00, 1000.13batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 10, Loss: 0.35861867666244507, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 4/4 [00:00<00:00, 500.33batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 11, Loss: 0.5755308866500854, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 4/4 [00:00<00:00, 999.71batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 12, Loss: 0.835973858833313, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 4/4 [00:00<00:00, 999.95batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 13, Loss: 1.053287148475647, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 4/4 [00:00<00:00, 614.64batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 14, Loss: 1.1874035596847534, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 4/4 [00:00<00:00, 888.34batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 15, Loss: 1.2355836629867554, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 4/4 [00:00<00:00, 499.68batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 16, Loss: 1.2151252031326294, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 4/4 [00:00<00:00, 1000.07batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 17, Loss: 1.1502166986465454, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 4/4 [00:00<00:00, 999.83batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 18, Loss: 1.063521146774292, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 4/4 [00:00<00:00, 999.95batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 19, Loss: 0.9721119403839111, Accuracy: 0.9814814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 4/4 [00:00<00:00, 999.95batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 20, Loss: 0.8867571353912354, Accuracy: 0.9814814925193787\n"
     ]
    }
   ],
   "source": [
    "# Run tensorflow session\n",
    "history = {'val_acc':[], 'val_loss':[]}\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Running the training in batches \n",
    "    batch_count = int(math.ceil(len(train_inputs)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_inputs = train_inputs[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "            # Run optimizer\n",
    "            _ = sess.run(optimizer, feed_dict={inputs: batch_inputs, labels: batch_labels})\n",
    "\n",
    "        # Check accuracy against validation data\n",
    "        val_accuracy, val_loss = sess.run([accuracy, loss], feed_dict={inputs: val_inputs, labels: val_labels})\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(\"After epoch {}, Loss: {}, Accuracy: {}\".format(epoch_i+1, val_loss, val_accuracy))\n",
    "\n",
    "    g = tf.get_default_graph()\n",
    "    saver.save(sess, './testsave')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [28.175617, 2.3828106, 1.8865613, 0.55091995, 4.8355646, 4.3576365, 1.5449842, 0.4649545, 0.27950451, 0.35861868, 0.57553089, 0.83597386, 1.0532871, 1.1874036, 1.2355837, 1.2151252, 1.1502167, 1.0635211, 0.97211194, 0.88675714], 'val_acc': [0.72222221, 0.98148149, 0.98148149, 1.0, 0.98148149, 0.98148149, 0.98148149, 1.0, 1.0, 1.0, 1.0, 0.98148149, 0.98148149, 0.98148149, 0.98148149, 0.98148149, 0.98148149, 0.98148149, 0.98148149, 0.98148149]}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'display' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-09900106b01d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display' is not defined"
     ]
    }
   ],
   "source": [
    "print(history)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.DataFrame(history)\n",
    "#display(df)\n",
    "\n",
    "df.plot(y=['val_loss'], figsize=(16,4), title='Loss')\n",
    "df.plot(y=['val_acc'], figsize=(16,4), title='Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(prediction, file_name = 'cat.jpg'):\n",
    "    #try: \n",
    "    #    file_name = 'cat.jpg'\n",
    "    #except IndexError:\n",
    "    #    print ('please enter image file path.........')\n",
    "    #    exit()\n",
    "    image_input = getvector(file_name).reshape((1,2048))\n",
    "    if 'cat' in file_name:\n",
    "        image_label = [[1, 0]]\n",
    "    else:\n",
    "        image_label = [[0, 1]]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        new_saver = tf.train.import_meta_graph('testsave.meta')\n",
    "        new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "\n",
    "        pred = sess.run(prediction, feed_dict={inputs: image_input})\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading inception_v3_2016_08_28.tar.gz 46.9%"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "pred = predict(prediction, 'cat.jpg')\n",
    "print('Predict: cat.jpg\\n', )\n",
    "img=mpimg.imread('cat.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "print ('It\\'s a cat: {}, It\\'s a dog: {}'.format(pred[0][0], pred[0][1]))\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "pred = predict(prediction, 'dog.jpg')\n",
    "print('Predict: dog.jpg\\n', )\n",
    "img=mpimg.imread('dog.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "print ('It\\'s a cat: {}, It\\'s a dog: {}'.format(pred[0][0], pred[0][1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
